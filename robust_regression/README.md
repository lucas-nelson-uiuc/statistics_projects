# README

In the FALL 2020 semesters, a few classmates and I in STAT 425 (Applied Regression and Design) explored robust regression methods and the specific applications said methods could have on our ability to derive conclusions on data sets that inherently violated homoscedasiticity and presented high leverage observations. We utilized a wage data set of 2013 Los Angeles public workers, utilizing position-based, department-based, salary-based, benefits-based statistics to determine the influence of outliers on a model's predictive capabilities. We conclude that robust regression models are superior to their linear regression counterparts given the situations we were presented with (see violations above); however, as a relatively new method in (yet, another) relatively new field of science, I would recommend taking our paper with a grain of salt, as we statisticians and others continue to explore the proper applications of robust regression.

# ABSTRACT

In recent decades, statistical methods have rapidly evolved to account for and make purpose of the influence of outliers in linear regression models. Whereas least squares regression operates under a set of limiting assumptions, robust regression accounts for and operates under violations of said assumptions, namely normality and homoscedasticity. We explore the influence of outliers and performance of regression models as a function of 2013-2016 payroll data of Los Angeles public employees. Typical of United States wage data, public employees tend to gather heavily around the median, allowing their senior colleagues to share the few but far between positions at the upper-end of this distribution. Conditional on a data set with inherent outliers, we find that although not all robust regression methods are created equally, their performance is indicative of their computational power and statistical significance when compared to linear regression models, especially if a distribution does not follow elementary ideals.
